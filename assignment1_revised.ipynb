{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cdf8b4",
   "metadata": {},
   "source": [
    "# Assignment #1\n",
    "\n",
    "## (Introduction)\n",
    "\n",
    "For this assignment, I wanted to take the opportunity to improve my Lisp skills by challenging myself to only use Lisp for the entirety of this project. I do believe there are many packages for statistical and machine learning available for Lisp through avenues such as Quicklisp (a repository for packages, similar to PyPI) and I will want to use them for more complicated problems, however, the best way to learn the language is to actually build something useful and sufficiently complicated so as to explore the 80% of the features which are used 20% of the time (Pareto principle.) Of course, applying my learning to the area I want to use the language (ML, NLP, etc.) makes the learning much more fit-to-purpose and some skills, such as processing the json data and manipulating the tokens, will be applicable even when using packages. Each of the cells will complete some aspect of this program, and each should be run in sequence (or run all at once, if you prefer). Some cells are separated out so that you can play with them and try to enter your own data or parameters.\n",
    "\n",
    "Throughout the notebook, I will indicate difficult programming issues that came up, how I approached solving them, and in cases where I utilized AI to help learn the language, I will indicate the prompts and general outputs, or in the interest of time, a summary of the interaction, the trajectory of the code improvements, and the final result.\n",
    "\n",
    "For the model evaluation aspect of this assignment, I will go into the various choices of model, setup and evaluation, some key insights and takeaways. Specifically:\n",
    " - Feature engineering / data choices (reasoning)\n",
    "    - consider out of distribution words, word negation, etc.\n",
    " - Implementation of at least 3 models (possibly variations of 1 kind of model)\n",
    " - Evaluation of models\n",
    "    - metrics (acc, F1, precision, recall)\n",
    "    - more robust?\n",
    "    - time complexity, space?\n",
    "    - general insights\n",
    " - Writing and Presentation\n",
    "    - (throughout this notebook, discussion)\n",
    "    - slides on the work in these evaluations / programming\n",
    "    - why I chose different models,\n",
    "    - practical considerations for these models, deployment\n",
    "\n",
    "Finally, similar evaluation will be performed for a second, minor task on entity recognition.\n",
    "\n",
    "##  (Part 1.1) Introduction to Naive Bayes Classifier From Scratch in Common Lisp (SBCL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50d381",
   "metadata": {
    "vscode": {
     "languageId": "commonlisp"
    }
   },
   "outputs": [],
   "source": [
    ";;;; quick install all our needed libraries\n",
    "(ql:quickload :cl-json)\n",
    "(ql:quickload :cl-ppcre)\n",
    "(ql:quickload :alexandria)\n",
    "\n",
    ";;;; setup our packages and namespace\n",
    "(defpackage :mine (:use :cl :alexandria))\n",
    "(in-package :mine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0ce0a",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "In developing this from scratch implementation, I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1b520",
   "metadata": {
    "vscode": {
     "languageId": "commonlisp"
    }
   },
   "outputs": [],
   "source": [
    ";;;; utility functions definitions\n",
    "\n",
    "(defun split-set (split-string raw-json)\n",
    "\"Takes the split string, which looks at the DATA_SPLIT field, \n",
    "and gathers the TEXT and INTENT from the student turns. for the split.\n",
    "Splits are 'test', 'validation', and 'train.'\"\n",
    " (let ((split))\n",
    " (loop :for item :in raw-json\n",
    " :when (string= split-string (cdar (cddr item)))                 ;; this is \"test\",\"train\",or \"validation\"\n",
    " do (setq split (append split \n",
    "    (loop :for turns :in (cdar (cdddr item))                    ;; this is a set of turns in a given split\n",
    "     :when (string= \"student\" (cdar (nthcdr 10 turns)))          ;; this checks the role of the turn, we want student.\n",
    "     :collect (list (cdar turns) (cdar (nthcdr 4 turns))))))     ;; (cdar turns) ;; the text\n",
    "                                                                ;; (cdar (nthcdr 4 turns)) ;; intent\n",
    " :finally (return split))))\n",
    "\n",
    ";; This function was chatGPT assisted, by the following prompt / query:\n",
    ";; \"I'm doing machine learning for intent classification with common lisp. How can I \n",
    ";; count up the class labels from the dataset? I do not know the different number of classes.\"\n",
    "(defun count-labels (dataset)\n",
    "  \"Count how many times each class label appears in DATASET.\n",
    "   DATASET is a list of (input label) pairs, or just labels.\"\n",
    "  (let ((counts (make-hash-table :test 'equalp)))\n",
    "    (dolist (item dataset)\n",
    "      ;; if dataset items are just labels, use ITEM directly\n",
    "      ;; if they are (input label) pairs, use (second item)\n",
    "      (let* ((label (if (consp item) (second item) item))\n",
    "             (current (gethash label counts 0)))\n",
    "        (setf (gethash label counts) (1+ current))))\n",
    "    counts))\n",
    "\n",
    ";; This function was chatGPT assisted, using the following prompt / query:\n",
    ";; \"I need a series of regex expressions for tokenizing English text.\"\n",
    "(defun tokenize (text)\n",
    "\"This function takes in text, processes it, and returns a list of tokens.\"\n",
    "  (let ((s text))\n",
    "    (setf s (cl-ppcre:regex-replace-all \"\\\\s+\" s \" \")) ;; white space collapse\n",
    "    (setf s (cl-ppcre:regex-replace-all \"([.,!?;:\\\"(){}\\\\[\\\\]])\" s \" \\\\1 \")) ;; punctuation\n",
    "    (setf s (cl-ppcre:regex-replace-all \"(\\\\w+)('ll|'re|'ve|n't|'s|'m|'d)\" s \"\\\\1 \\\\2\")) ;; contractions\n",
    "    (setf s (cl-ppcre:regex-replace-all \"(\\\\w+)-(\\\\w+)\" s \"\\\\1 - \\\\2\")) ;; dashes\n",
    "    (remove \"\" (cl-ppcre:split \"\\\\s+\" s))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ce348",
   "metadata": {},
   "source": [
    "## The Naive Bayes Classifier\n",
    "\n",
    "In this code, I define the classifier function. The definition of this function follows the \"let over lambda\" concept to encapsulate an environment in the returned function. Initially, the function takes in the training document, optionally printing some statistics, and then builds (\"trains\") the classifier. This is primarily made up of building hash tables to contain the classes, the priors, word tables for each class, and the overall vocabulary. The training data is tokenized inside the function as well.\n",
    "\n",
    "### The lambda function\n",
    "The purpose of the `naive-bayes-classifier` function is to build the training statistics, and return a function that can be called on some string, in other words, it is a function factory for making NBCs on a particular set of training data. The training data is set at the time of the function call, but the smoothing can be selected when calling the returned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d61be",
   "metadata": {
    "vscode": {
     "languageId": "commonlisp"
    }
   },
   "outputs": [],
   "source": [
    ";;;; build an NBC classifier which takes in a dataset and builds all the internal tracking,\n",
    ";;;; and returns a function which takes a sample string and returns the predicted class.\n",
    "\n",
    "(defun naive-bayes-classifier (train &optional (print-stats nil))\n",
    "\"NBC that takes the training data in the format of ('string of text' . 'label' )\n",
    "and returns a naive bayes classifer. \"\n",
    "(let ((vocab (make-hash-table :size 10000 :test 'equalp))\n",
    "      (priors (make-hash-table :test 'equalp))\n",
    "      (label-counts (make-hash-table :test 'equalp))\n",
    "      (word-counts (make-hash-table :test 'equalp))\n",
    "      (training-tokens (map 'list #'(lambda (n) (list (cadr n) (tokenize (car n)))) train)))\n",
    ";; get label counts (and so the keys for classes)\n",
    "      (setf label-counts (count-labels train))\n",
    ";; build prior\n",
    "      (loop for k being the hash-keys in label-counts using (hash-value v)\n",
    "      do (setf (gethash k priors) (/ (coerce v 'double-float) (length training-tokens))))\n",
    ";; build per-class word counts\n",
    "      ;; build a hash table for the words in each class.\n",
    "      (loop for k being the hash-keys in label-counts\n",
    "      do (setf (gethash k word-counts) (make-hash-table :size 1000 :test 'equalp)))\n",
    "\n",
    "      ;; go through the token class pairs, adding all words to vocab, and \n",
    "      ;; to the class hash tables\n",
    "      (dolist (pair training-tokens)\n",
    "            (destructuring-bind (label words) pair\n",
    "                  (dolist (w words)\n",
    "                        (incf (gethash w vocab 0))\n",
    "                        (incf (gethash w (gethash label word-counts) 0)))))\n",
    "\n",
    ";; if stats are requested, print them when creating the classifier\n",
    "(when print-stats\n",
    "(format t \"Class labels and counts: ~%\")\n",
    "(maphash #'(lambda (k v) (format t \"~S: ~D~%\" k v)) label-counts)\n",
    "(format t \"Class priors: ~%\")\n",
    "(maphash #'(lambda (k v) (format t \"~A: ~D~%\" k v)) priors)\n",
    "(format t \"Total words: ~D~%\" (hash-table-count vocab)))\n",
    "(maphash (lambda (k v)\n",
    "           (format t \"Label: ~A â†’ table ~A (~D words)~%\"\n",
    "                   k v (if v (hash-table-count v) -1)))\n",
    "         word-counts)\n",
    ";; make the classifier\n",
    "#'(lambda (command &rest args)\n",
    "\"Options are: predict (string and optional smoothing)\n",
    "              get-word-table (key)\n",
    "              get-vocab\n",
    "              get-priors\"\n",
    "   (ecase command\n",
    "\n",
    "      (:predict\n",
    "        (destructuring-bind (x &optional (smooth 0.0d0) (get-probs nil)) args\n",
    "            (let ((token-input (tokenize x))\n",
    "                   (results '()))\n",
    "                   (maphash #'(lambda (k wtable)\n",
    "                        (unless (hash-table-p wtable)\n",
    "                              (error \"class ~S has non-hash value ~S\" k wtable))\n",
    "                        (let* ((V (hash-table-count vocab))\n",
    "                               (WC (hash-table-count wtable))\n",
    "                               (denom (+ WC (* smooth V)))\n",
    "                               (likelihood (loop for w in token-input     \n",
    "                                                        ;; zero numerator means word not found; before smoothing.\n",
    "                                                        ;; zero denominator means that we have no words, basically.\n",
    "                                           summing (let ((num (gethash w wtable 0))) \n",
    "                                           (if (or (zerop denom) (zerop num))\n",
    "                                           0.0d0\n",
    "                                           (log (/ (+ smooth num) denom)))))))\n",
    "                        (push (list k (+ (log (max 1 (gethash k priors 0.0d0))) likelihood)) results)))\n",
    "                        word-counts)\n",
    "                  (if get-probs\n",
    "                  results\n",
    "                  (car (reduce #'(lambda (a b) (if (> (cadr a)(cadr b)) a b))\n",
    "                  results))))))\n",
    "            \n",
    "      (:get-word-table\n",
    "        (destructuring-bind (key) args\n",
    "         (gethash key word-counts)))\n",
    "         \n",
    "      (:get-vocab\n",
    "       vocab)\n",
    "       \n",
    "      (:get-priors\n",
    "       priors)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44d96d",
   "metadata": {
    "vscode": {
     "languageId": "commonlisp"
    }
   },
   "outputs": [],
   "source": [
    ";;;; setup and prepare our data here.\n",
    "(defvar *data-location* \"./data\")\n",
    ";; the raw json text\n",
    "(defvar *json-text* nil)\n",
    "\n",
    ";; all our splits from the json text\n",
    "(defvar *test-list* nil)\n",
    "(defvar *train-list* nil)\n",
    "(defvar *valid-list* nil)\n",
    "\n",
    ";; load the json file and preprocess data into the splits.\n",
    "(setf *json-text* (cl-json:decode-json-from-source (open (concatenate 'string *data-location* \"/wizard_of_tasks_cooking_v1.0.json\"))))\n",
    "(setf *test-list* (split-set \"test\" *json-text*))\n",
    "(setf *train-list* (split-set \"train\" *json-text*))\n",
    "(setf *valid-list* (split-set \"validation\" *json-text*))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30103bed",
   "metadata": {
    "vscode": {
     "languageId": "commonlisp"
    }
   },
   "outputs": [],
   "source": [
    ";; get a classifer.\n",
    "(defvar *nbc* (naive-bayes-classifier *train-list* T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915214f",
   "metadata": {
    "vscode": {
     "languageId": "commonlisp"
    }
   },
   "outputs": [],
   "source": [
    ";; nbc without smoothing\n",
    "(let ((cls (funcall *nbc* :predict \"Can you tell me what I should do next?\" 0.2d0)))\n",
    "    (format t \"the predicted class is: ~A ~%\" cls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Common Lisp",
   "language": "common-lisp",
   "name": "common-lisp"
  },
  "language_info": {
   "codemirror_mode": "text/x-common-lisp",
   "file_extension": ".lisp",
   "mimetype": "text/x-common-lisp",
   "name": "common-lisp",
   "pygments_lexer": "common-lisp",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
